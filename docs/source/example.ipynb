{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sym_tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "import sym_tensor.ops as st_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "(Links work only when you run the notebook)\n",
    "- [Tensor basics](#Tensor-basics)\n",
    "- [U(1) symmetry](#U(1)-symmetry)\n",
    "- [Differentiable programming](#Differentiable-programming)\n",
    "- [Gradient descent demo](#Gradient-descent-demo) (new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor basics\n",
    "A symmetric tensor is defined by its elements, quantum numbers (`charges`) and symmetry group (`ZN`). \n",
    "To initialize a `Z2`-symmetric matrix, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-D SymTensor (4, 4):\n",
      "|  Free elements: 8 / 16 (50% of total) in 2 blocks \n",
      "|  ZN: 2 \n",
      "|  Totalcharge: 0 \n",
      "|  Readorder: [0, 1] \n",
      " \n",
      "Quantum numbers on each leg:\n",
      "                 +-----+\n",
      "( 0): [ 0, 1 ] --|     |-- ( 1): [ 0, 1 ]\n",
      "                 +-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "charges = [\n",
    "  [\n",
    "    [0,2],\n",
    "    [1,2]\n",
    "  ],[\n",
    "    [0,2],\n",
    "    [1,2]\n",
    "  ]\n",
    "]\n",
    "\n",
    "# Use elements=None to initialize with random elements\n",
    "T = sym_tensor.newtensor(elements=None, charges=charges, ZN=2)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Z2`-symmetric matrix can be represented in block-diagonal form; in this case by two 2x2 blocks. Every symmetric tensor can be reshaped to a block-diagonal matrix by fusing indices together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8911, 0.0553],\n",
      "        [0.0132, 0.3870]])\n",
      "tensor([[0.9066, 0.0323],\n",
      "        [0.6183, 0.6328]])\n",
      "tensor([[0.8911, 0.0553, 0.0000, 0.0000],\n",
      "        [0.0132, 0.3870, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9066, 0.0323],\n",
      "        [0.0000, 0.0000, 0.6183, 0.6328]])\n"
     ]
    }
   ],
   "source": [
    "# Choose [0] as the left index, [1] as the right index and 'n' (irrelevant for Z2)\n",
    "blocks, *meta = T.get_blocks(left_legs=[0], right_legs=[1], side='n') # The metadata is necessary to reverse the reshaping\n",
    "for b in blocks:\n",
    "    print(b)\n",
    "T_full = T.to_full() # Embed the blocks in a matrix (only Z2 matrices for now supported)\n",
    "print(T_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To multiply tensors together, there are several options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7947, 0.0707, 0.0000, 0.0000],\n",
      "        [0.0169, 0.1505, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8420, 0.0497],\n",
      "        [0.0000, 0.0000, 0.9518, 0.4204]])\n",
      "tensor([[0.7947, 0.0707, 0.0000, 0.0000],\n",
      "        [0.0169, 0.1505, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8420, 0.0497],\n",
      "        [0.0000, 0.0000, 0.9518, 0.4204]])\n",
      "tensor([[0.7947, 0.0707, 0.0000, 0.0000],\n",
      "        [0.0169, 0.1505, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8420, 0.0497],\n",
      "        [0.0000, 0.0000, 0.9518, 0.4204]])\n",
      "tensor([[0.7947, 0.0707, 0.0000, 0.0000],\n",
      "        [0.0169, 0.1505, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8420, 0.0497],\n",
      "        [0.0000, 0.0000, 0.9518, 0.4204]])\n"
     ]
    }
   ],
   "source": [
    "T2 = T.copy() # No shared memory\n",
    "\n",
    "# Ncon\n",
    "res = st_ops.ncon([T,T2], ([-1,1],[1,-2]))\n",
    "print(res.to_full())\n",
    "\n",
    "# Mult (arguments are the legs to be contracted on each tensor)\n",
    "res = T.mult(T2, [1], [0])\n",
    "print(res.to_full())\n",
    "\n",
    "# Matrix product (in general: contract over last index of T and first index of T2)\n",
    "res = T @ T2\n",
    "print(res.to_full())\n",
    "\n",
    "# Full result\n",
    "print(T_full @ T_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8911, 0.0553, 0.0000, 0.0000],\n",
      "        [0.0132, 0.3870, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9066, 0.0323],\n",
      "        [0.0000, 0.0000, 0.6183, 0.6328]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# SVD (left_legs, right_legs, number of singular values to keep, absorb s left/right/not)\n",
    "u,s,v = T.svd([0], [1], n=np.inf, absorb='n')\n",
    "print((u@s@v.t()).to_full()) # Equal to T itself\n",
    "print(T.allclose(u@s@v.t()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U(1) symmetry\n",
    "The continuous `U(1)` symmetry can be approximated by a `ZN` symmetry with large enough `N`. Initialize a tensor like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-D SymTensor (2, 3, 3):\n",
      "|  Free elements: 4 / 18 (22% of total) in 4 blocks \n",
      "|  ZN: 36 \n",
      "|  Totalcharge: 0 \n",
      "|  Readorder: [0, 1, 2] \n",
      " \n",
      "Quantum numbers on each leg:\n",
      "                     +-----+\n",
      "( 0):    [ 1, 35 ] --|     |-- ( 2): [ 0, 1, 35 ]\n",
      "( 1): [ 0, 1, 35 ] --|     |\n",
      "                     +-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "charges = [\n",
    "  [\n",
    "    [1,1],\n",
    "    [35,1]\n",
    "  ],[\n",
    "    [0,1],\n",
    "    [1,1],\n",
    "    [35,1]\n",
    "  ],[\n",
    "    [0,1],\n",
    "    [1,1],\n",
    "    [35,1]\n",
    "  ]\n",
    "]\n",
    "\n",
    "# Use elements=None to initialize with random elements\n",
    "T = sym_tensor.newtensor(elements=None, charges=charges, ZN=36)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum number:  0\n",
      "tensor([[0.6956],\n",
      "        [0.9434]])\n",
      "Quantum number:  1\n",
      "tensor([[0.0927]])\n",
      "Quantum number:  35\n",
      "tensor([[0.7140]])\n"
     ]
    }
   ],
   "source": [
    "blocks, *meta = T.get_blocks(left_legs=[0,1], right_legs=[2], side='n') # The metadata is necessary to reverse the reshaping\n",
    "for i,b in enumerate(blocks):\n",
    "    if b.numel() > 0:\n",
    "        print(\"Quantum number: \", i)\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same information can be obtained by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix block structure with combined indices (0, 1) x (2,)\n",
      "  0: Size (2, 1)     block with quantum numbers [0, 0]     (2    elements)\n",
      "  1: Size (1, 1)     block with quantum numbers [1, 1]     (1    elements)\n",
      "  2: Size (1, 1)     block with quantum numbers [35, 35]   (1    elements)\n",
      "---->    4 elements in total\n",
      "Tensor block structure (4 blocks)\n",
      "  0 (  3): Size (1, 1, 1) block with quantum numbers [35, 1, 0]           (1   elements)\n",
      "  1 (  4): Size (1, 1, 1) block with quantum numbers [1, 35, 0]           (1   elements)\n",
      "  2 (  7): Size (1, 1, 1) block with quantum numbers [35, 0, 1]           (1   elements)\n",
      "  3 ( 12): Size (1, 1, 1) block with quantum numbers [1, 0, 35]           (1   elements)\n",
      "---->    4 elements in total\n"
     ]
    }
   ],
   "source": [
    "# Reshaped as matrix with indices 0 and 1 fused\n",
    "T.show_block_structure([0,1], [2])\n",
    "\n",
    "# As tensor\n",
    "T.show_block_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian initizalization (automatic)\n",
    "For the automatic conversion of a Hamiltonian (or other operators) in full array format to a symmetric tensor, we can use the `symmetrize_operator` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of H:  tensor([ 0.2500,  0.5000, -0.2500, -0.2500,  0.5000,  0.2500])\n",
      "4-D SymTensor (2, 2, 2, 2):\n",
      "|  Free elements: 6 / 16 (37% of total) in 6 blocks \n",
      "|  ZN: 36 \n",
      "|  Totalcharge: 0 \n",
      "|  Readorder: [0, 1, 2, 3] \n",
      " \n",
      "Quantum numbers on each leg:\n",
      "                  +-----+\n",
      "( 0): [ 1, 35 ] --|     |-- ( 2): [ 1, 35 ]\n",
      "( 1): [ 1, 35 ] --|     |-- ( 3): [ 1, 35 ]\n",
      "                  +-----+\n",
      "\n",
      "Tensor block structure (6 blocks)\n",
      "  0 (  3): Size (1, 1, 1, 1) block with quantum numbers [35, 35, 1, 1]       (1   elements)\n",
      "  1 (  5): Size (1, 1, 1, 1) block with quantum numbers [35, 1, 35, 1]       (1   elements)\n",
      "  2 (  6): Size (1, 1, 1, 1) block with quantum numbers [1, 35, 35, 1]       (1   elements)\n",
      "  3 (  9): Size (1, 1, 1, 1) block with quantum numbers [35, 1, 1, 35]       (1   elements)\n",
      "  4 ( 10): Size (1, 1, 1, 1) block with quantum numbers [1, 35, 1, 35]       (1   elements)\n",
      "  5 ( 12): Size (1, 1, 1, 1) block with quantum numbers [1, 1, 35, 35]       (1   elements)\n",
      "---->    6 elements in total\n",
      "\n",
      "Elements of H:  tensor([ 0.2500,  0.5000, -0.2500, -0.2500,  0.5000,  0.2500])\n",
      "4-D SymTensor (2, 2, 2, 2):\n",
      "|  Free elements: 6 / 16 (37% of total) in 6 blocks \n",
      "|  ZN: 36 \n",
      "|  Totalcharge: 0 \n",
      "|  Readorder: [0, 1, 2, 3] \n",
      " \n",
      "Quantum numbers on each leg:\n",
      "                  +-----+\n",
      "( 0): [ 1, 35 ] --|     |-- ( 2): [ 1, 35 ]\n",
      "( 1): [ 1, 35 ] --|     |-- ( 3): [ 1, 35 ]\n",
      "                  +-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "H_Heis = np.array([\n",
    "    [0.25, 0, 0, 0],\n",
    "    [0, -0.25, 0.5, 0],\n",
    "    [0, 0.5, -0.25, 0],\n",
    "    [0, 0, 0, 0.25]\n",
    "])\n",
    "\n",
    "# These represent the quantum numbers on each physical index\n",
    "# For a spin-1/2 system with U(1) symmetry (approximated by Z36), the \n",
    "# basis states (up,down) correspond to quantum numbers (1,-1) mod ZN = (1,35)\n",
    "phys_charges = [1,35]\n",
    "\n",
    "# Either give the operator in matrix-form (bra indices, ket indices):\n",
    "H_Heis_symmetric = sym_tensor.symmetrize_operator(op=H_Heis, phys_charges=phys_charges, ZN=36)\n",
    "print('Elements of H: ', H_Heis_symmetric.data)\n",
    "print(H_Heis_symmetric)\n",
    "H_Heis_symmetric.show_block_structure()\n",
    "\n",
    "# Or in tensor format, with an index for each physical leg:\n",
    "H_Heis = np.reshape(H_Heis, [2,2,2,2])\n",
    "H_Heis_symmetric = sym_tensor.symmetrize_operator(op=H_Heis, phys_charges=phys_charges, ZN=36)\n",
    "print('\\nElements of H: ', H_Heis_symmetric.data)\n",
    "print(H_Heis_symmetric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariant tensors\n",
    "If you want to, for example, write a spin-flip operator as SymTensor, it will not be possible with a `U(1)`-invariant tensor, since it contains exactly the elements that are not allowed by the symmetry.\n",
    "In order to still make such a SymTensor, you can attach an extra index to the tensor that carries a nontrivial quantum number, such that the total charge `Q_in - Q_out` is still conserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S+ operator:\n",
      "[[0 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "Splus = np.array([[0,1],[0,0]])\n",
    "print(\"S+ operator:\")\n",
    "print(Splus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SymTensor will have only two elements that are allowed to be nonzero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General operator:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "2-D SymTensor (2, 2):\n",
      "|  Free elements: 2 / 4 (50% of total) in 2 blocks \n",
      "|  ZN: 36 \n",
      "|  Totalcharge: 0 \n",
      "|  Readorder: [0, 1] \n",
      " \n",
      "Quantum numbers on each leg:\n",
      "                  +-----+\n",
      "( 0): [ 1, 35 ] --|     |-- ( 1): [ 1, 35 ]\n",
      "                  +-----+\n",
      "\n",
      "tensor([1., 4.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "operator_el = np.array([[1,2],[3,4]])\n",
    "print(\"General operator:\")\n",
    "print(operator_el)\n",
    "print(\"\")\n",
    "\n",
    "op = sym_tensor.symmetrize_operator(operator_el, phys_charges=[1,35], ZN=36)\n",
    "# Tensor with 2 allowed nonzero elements, namely the diagonal elements\n",
    "# of the operator in matrix form\n",
    "print(op)\n",
    "print(op.data) # [1, 4]\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a new tensor with an additional index with quantum number +2 or -2\n",
    "Why ±2? Because in the code there are no fractional quantum numbers, so spin-up corresponds to +1 (not +1/2). The S+ operator changes spin-down (-1) to spin-up (+1), so the difference is +2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-D SymTensor (2, 2, 1):\n",
      "|  Free elements: 1 / 4 (25% of total) in 1 blocks \n",
      "|  ZN: 36 \n",
      "|  Totalcharge: 0 \n",
      "|  Readorder: [0, 1, 2] \n",
      " \n",
      "Quantum numbers on each leg:\n",
      "                  +-----+\n",
      "( 0): [ 1, 35 ] --|     |-- ( 2):     [ 2 ]\n",
      "( 1): [ 1, 35 ] --|     |\n",
      "                  +-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "charges_with_extra_leg = [[[1,1],[35,1]], [[1,1],[35,1]], [[2,1]]]\n",
    "op = sym_tensor.newtensor(charges=charges_with_extra_leg, ZN=36)\n",
    "\n",
    "# Note that now there's only a single allowed nonzero element - just like the \n",
    "# the S+ operator\n",
    "print(op)\n",
    "\n",
    "# Make it S+:\n",
    "op.data[0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way of doing this is by using the `totalcharge` property, which can be set to ±2 for this example. However this property is not yet fully supported in the code so that may cause some problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian initialization (manual)\n",
    "When building a Hamiltonian by hand with a nontrivial symmetry, the blocks of the tensor have be matched with the right elements of the Hamiltonian.\n",
    "Here we use a Heisenberg Hamiltonian as an example. The basis is the standard spin basis {(up,up), (up,down), (down,up), (down,down)}, which can be identified with the set of quantum numbers {(1,1), (1,-1), (-1,1), (-1,-1)}.\n",
    "The 4x4 (2-site) Hamiltonian can be reshaped to tensor form 2x2x2x2 where each index connects to a physical index of a tensor network. Each index then runs over the quantum numbers (1,-1) and will be given a `charge` of [[1,1],[35,1]] when we approximate the `U(1)` symmetry by `Z36`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25  0.    0.    0.  ]\n",
      " [ 0.   -0.25  0.5   0.  ]\n",
      " [ 0.    0.5  -0.25  0.  ]\n",
      " [ 0.    0.    0.    0.25]]\n"
     ]
    }
   ],
   "source": [
    "H_Heis = np.array([\n",
    "    [0.25, 0, 0, 0],\n",
    "    [0, -0.25, 0.5, 0],\n",
    "    [0, 0.5, -0.25, 0],\n",
    "    [0, 0, 0, 0.25]\n",
    "])\n",
    "print(H_Heis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to identify all elements with the right quantum numbers. The top-left element maps an (up, up) state to an (up, up) state, corresponding to quantum numbers ( (1,1), (1,1) ).\n",
    "The element on its bottom right corner (-0.25) maps (up,down) -> (up,down), so quantum numbers ( (1, -1), (1, -1) ).\n",
    "Let's make a SymTensor with (±1) charges on each leg and inspect the nonzero blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor block structure (6 blocks)\n",
      "  0 (  3): Size (1, 1, 1, 1) block with quantum numbers [35, 35, 1, 1]       (1   elements)\n",
      "  1 (  5): Size (1, 1, 1, 1) block with quantum numbers [35, 1, 35, 1]       (1   elements)\n",
      "  2 (  6): Size (1, 1, 1, 1) block with quantum numbers [1, 35, 35, 1]       (1   elements)\n",
      "  3 (  9): Size (1, 1, 1, 1) block with quantum numbers [35, 1, 1, 35]       (1   elements)\n",
      "  4 ( 10): Size (1, 1, 1, 1) block with quantum numbers [1, 35, 1, 35]       (1   elements)\n",
      "  5 ( 12): Size (1, 1, 1, 1) block with quantum numbers [1, 1, 35, 35]       (1   elements)\n",
      "---->    6 elements in total\n"
     ]
    }
   ],
   "source": [
    "H = sym_tensor.newtensor(charges=4*[[[1,1],[35,1]]], ZN=36)\n",
    "H.show_block_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 6 blocks of size 1. One important remark is that the charges on the outgoing indices [2,3] are *conjugated* (in the code the 'arrows' are always pointed inwards), so in the first block [35,35,1,1] corresponds to ( (-1, -1), (-1, -1) ), which is the element that maps (down,down)->(down,down) - the element on the bottom right of the Hamiltonian (0.25).\n",
    "We can then identify each block with a matrix element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of H:  tensor([ 0.2500,  0.5000, -0.2500, -0.2500,  0.5000,  0.2500])\n",
      "Matrix block structure with combined indices (0, 1) x (2, 3)\n",
      "  0: Size (2, 2)     block with quantum numbers [0, 0]     (4    elements)\n",
      "  1: Size (1, 1)     block with quantum numbers [2, 2]     (1    elements)\n",
      "  2: Size (1, 1)     block with quantum numbers [34, 34]   (1    elements)\n",
      "---->    6 elements in total\n"
     ]
    }
   ],
   "source": [
    "data = torch.zeros(6)\n",
    "data[0] = H_Heis[3,3] # (down,down) -> (down,down)\n",
    "data[1] = H_Heis[2,1] # (up,down)   -> (down,up)\n",
    "data[2] = H_Heis[1,1] # (up,down)   -> (up,down)\n",
    "data[3] = H_Heis[2,2] # (down,up)   -> (down,up)\n",
    "data[4] = H_Heis[1,2] # (down,up)   -> (up,down)\n",
    "data[5] = H_Heis[0,0] # (up,up)     -> (up,up)\n",
    "H.data = data\n",
    "print('Elements of H: ', H.data)\n",
    "H.show_block_structure(left_legs=[0,1], right_legs=[2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable programming\n",
    "Within Torch, all tensor operations should be differentiable in order to obtain gradients. Turn on the tracking of gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-D SymTensor (2, 3, 3):\n",
      "|  Free elements: 4 / 18 (22% of total) in 4 blocks \n",
      "|  ZN: 36 \n",
      "|  Totalcharge: 0 \n",
      "|  Readorder: [0, 1, 2] \n",
      "|  Requires gradient: True \n",
      " \n",
      "Quantum numbers on each leg:\n",
      "                     +-----+\n",
      "( 0):    [ 1, 35 ] --|     |-- ( 2): [ 0, 1, 35 ]\n",
      "( 1): [ 0, 1, 35 ] --|     |\n",
      "                     +-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T.requires_grad = True\n",
    "print(T) # Note that now the gradient will be tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-D SymTensor (1,):\n",
      "|  Free elements: 1 / 1 (100% of total) in 1 blocks \n",
      "|  ZN: 36 \n",
      "|  Totalcharge: 0 \n",
      "|  Readorder: [0] \n",
      "|  Requires gradient: True \n",
      "|  Gradient function: <torch.autograd.function.MultBackward object at 0x120c8b458> \n",
      " \n",
      "Quantum numbers on each leg:\n",
      "              +-----+\n",
      "( 0): [ 0 ] --|     |\n",
      "              +-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T2 = sym_tensor.newtensor(elements=None, charges=charges, ZN=36)\n",
    "T2 = T2.conj() # Take the hermitian conjugate\n",
    "nrm = T.mult(T2, [0,1,2], [0,1,2]) # Full contraction to a scalar\n",
    "print(nrm) # Note that now the gradient function has been stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9855, 0.4243, 0.9606, 0.6537])\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation is the same as with regular torch Tensors\n",
    "nrm.backward()\n",
    "print(T.grad) # The gradient will be stored as a raw torch Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods defined for SymTensor objects automatically determine whether they should be wrapped into a Torch autograd function (defined in `ops.py`) by checking the `T.requires_grad` property.\n",
    "A method which is differentiable follows this general structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# tensors.py\n",
    "# wrap_grad decorator wraps function in autograd operation if necessary \n",
    "# ('Max' in ops.py in this case)\n",
    "# The decorator can be bypassed by adding the nograd=True keyword argument \n",
    "# T.max(nograd = True)\n",
    "# or by calling the equivalent underscore variant:\n",
    "# T.max_()\n",
    "# When the tensor does not have the property requireds_grad==True, the gradient\n",
    "# is never computed, so the explicit bypass is almost never necessary.\n",
    "@_Decorators.wrap_grad(st_ops.Max)\n",
    "def max(self): # Implementation on tensor level\n",
    "    \"\"\" Largest element \"\"\"\n",
    "    max_el = self.data.max().unsqueeze(0).detach().clone()\n",
    "    T = newtensor(elements=None, charges=[[[0,1]]], ZN=self.ZN, totalcharge=self.totalcharge)\n",
    "    T.data = max_el\n",
    "    return T\n",
    "\n",
    "# ops.py\n",
    "class Max(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor):\n",
    "        res = tensor.max_() # Basic operation on tensor (note the underscore)\n",
    "        \n",
    "        # Save information for backward pass\n",
    "        ctx.intermediate_results = (tensor, res)\n",
    "        \n",
    "        # In some operations, only the metadata of the SymTensor needs to \n",
    "        # be saved, since Torch already saves the elements of the output.\n",
    "        # res.meta contains all information of the resulting SymTensor \n",
    "        # (except the elements) that can be used to reconstruct a SymTensor\n",
    "        # in the backward pass\n",
    "        # Saving the information would then be something like:\n",
    "        # ctx.intermediate_results = (res.meta)\n",
    "        \n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass, which receives the grad_output in the form of a \n",
    "        # regular Torch Tensor. If necessary, we cast the grad_output back\n",
    "        # to a SymTensor like this:\n",
    "        # (meta) = ctx.intermediate_results\n",
    "        # tensor = tensors.from_meta(meta, elements=grad_output)\n",
    "        (tensor, res) = ctx.intermediate_results\n",
    "        tensor = tensor.copy()\n",
    "        new_data = torch.zeros_like(tensor.data)\n",
    "        tensor.data = new_data.masked_fill(tensor.data == res.data, grad_output.squeeze())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the elementwise functions can be simply be implemented by calling the equivalent Torch function on the elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __add__(self, other):\n",
    "    return self._elem_function(torch.Tensor.__add__, other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`T._elem_function` makes sure the elements are in the correct order, then calls the corresponding torch.Tensor function (here `torch.Tensor.__add__`) on the elements and finally calls `T.fill_data` to reconstruct the SymTensor from the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent demo\n",
    "We can put the SymTensors to work in a simple 2-site Heisenberg chain example. \n",
    "In this case, there are two variable tensors, for which we construct (1) a torch.nn.Parameter with the elements and (2) a corresponding SymTensor that is filled with the elements and used in the rest of the code.\n",
    "Note that since `T.fill_data` is differentiable, the gradient on the SymTensors are automatically propagated to the Parameter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "from torch import nn\n",
    "class TwoSiteHeis(nn.Module):\n",
    "    def __init__(self, elems_A=None, elems_B=None):\n",
    "        super(TwoSiteHeis, self).__init__()\n",
    "        H_Heis = np.array([\n",
    "            [0.25, 0, 0, 0],\n",
    "            [0, -0.25, 0.5, 0],\n",
    "            [0, 0.5, -0.25, 0],\n",
    "            [0, 0, 0, 0.25]\n",
    "            ])\n",
    "        phys_charges = [1,35]\n",
    "        self.H = sym_tensor.symmetrize_operator(op=H_Heis, phys_charges=phys_charges, ZN=36)\n",
    "        \n",
    "        # A and B are simple U(1) MPS tensors that form a two-site chain\n",
    "        ch_A = ch_B = [[[1,1],[35,1]], [[0,1],[1,1],[35,1]]]\n",
    "        self.A = sym_tensor.newtensor(elements=elems_A, charges=ch_A, ZN=36)\n",
    "        self.B = sym_tensor.newtensor(elements=elems_B, charges=ch_B, ZN=36)\n",
    "\n",
    "        if elems_A is None:\n",
    "            self.elems_A = nn.Parameter(self.A.data)\n",
    "        if elems_B is None:\n",
    "            self.elems_B = nn.Parameter(self.B.data)\n",
    "\n",
    "    def forward(self, elems_A=None, elems_B=None):\n",
    "        # The elements can be given as optional arguments so that the gradient \n",
    "        # of the full forward pass can be checked numerically\n",
    "        # Normally, just take the tensors that are stored in the model\n",
    "        if elems_A is not None:\n",
    "            self.elems_A = elems_A\n",
    "        if elems_B is not None:\n",
    "            self.elems_B = elems_B\n",
    "\n",
    "        # This is an important step: it copies the elements into the SymTensor \n",
    "        # in such a way that the operation remains differentiable\n",
    "        # In the backward step, the gradient of A (SymTensor) will be propagated \n",
    "        # back to the gradient of elems_A (regular Torch Tensor)\n",
    "        A = self.A.fill_data(self.elems_A)\n",
    "        B = self.B.fill_data(self.elems_B)\n",
    "\n",
    "        A_c = A.conj()\n",
    "        B_c = B.conj()\n",
    "        E = st_ops.ncon([A, B, self.H, A_c, B_c], ([1,2],[3,2],[1,3,4,5],[4,6],[5,6]))\n",
    "        nrm = st_ops.ncon([A, B, A_c, B_c], ([1,2],[3,2],[1,4],[3,4]))\n",
    "        E_normalized = E / nrm\n",
    "        return E_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting energy: tensor([-0.1177])\n",
      "Gradient of A: tensor([ 0.2022, -0.2456])\n",
      "Gradient of SymTensor m.A: None\n",
      "Gradient check: True\n"
     ]
    }
   ],
   "source": [
    "m = TwoSiteHeis()\n",
    "\n",
    "# Check the forward pass with two random initial tensors\n",
    "elems_A = torch.nn.Parameter(torch.rand(m.A.numel(), requires_grad=True))\n",
    "elems_B = torch.nn.Parameter(torch.rand(m.B.numel(), requires_grad=False))\n",
    "E = m.forward(elems_A, elems_B) # Returns the energy in a SymTensor\n",
    "print(\"Starting energy:\", E.data)\n",
    "\n",
    "# Notice that now the gradient of the SymTensor that is stored \n",
    "# on the model (m.A) is propagated to the Parameter object (elems_A)\n",
    "E.backward()\n",
    "print(\"Gradient of A:\", elems_A.grad)\n",
    "# Since the SymTensor is not a leaf in the graph, its gradient will be\n",
    "# deleted, unless you set m.A.retain_grad()\n",
    "print(\"Gradient of SymTensor m.A:\", m.A.grad) # None\n",
    "\n",
    "# Check the gradient of the full computation against a numerical gradient\n",
    "print(\"Gradient check:\", torch.autograd.gradcheck(m, (elems_A, elems_B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "[Parameter containing:\n",
      "tensor([0.1730, 0.4866], requires_grad=True), Parameter containing:\n",
      "tensor([0.2344, 0.4912], requires_grad=True)]\n",
      "Step 0 energy tensor([0.2291])\n",
      "Step 1 energy tensor([-0.7500])\n",
      "Step 2 energy tensor([-0.7500])\n",
      "Step 3 energy tensor([-0.7500])\n",
      "Step 4 energy tensor([-0.7500])\n",
      "Step 5 energy tensor([-0.7500])\n",
      "Step 6 energy tensor([-0.7500])\n",
      "Step 7 energy tensor([-0.7500])\n",
      "Step 8 energy tensor([-0.7500])\n",
      "Step 9 energy tensor([-0.7500])\n",
      "Final energy: tensor([-0.7500]) error: tensor([2.3562e-12])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# main simulation script\n",
    "m = TwoSiteHeis()\n",
    "print(\"Model Parameters:\")\n",
    "print(list(m.parameters()))\n",
    "\n",
    "learning_rate = 1\n",
    "\n",
    "optimizer = torch.optim.LBFGS(m.parameters(), max_iter=10, lr=learning_rate)\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = m.forward()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "energies = []\n",
    "for epoch in range(10):\n",
    "    loss = optimizer.step(closure)\n",
    "    print(\"Step\", epoch, \"energy\", loss.data)\n",
    "    energies.append(loss)\n",
    "\n",
    "print(\"Final energy:\", energies[-1].data, \"error:\", energies[-1].data--0.75)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(energies, '-+');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
